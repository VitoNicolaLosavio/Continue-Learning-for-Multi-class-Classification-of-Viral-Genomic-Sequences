{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading of the dataset and First preprocessing step\n",
    "for this project I chose the dataset available on Hugging face ncbi-virus-complete-dna-v230722, but reducing its size, the original dataset contains over 2 million rows, but for a matter of complexity and capacity of the pc I took a portion containing 200 000 items, "
   ],
   "id": "13198ea999ae2fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To download the complete dataset, you can run the cell below which will automatically check if the dataset is in the folder, otherwise the download",
   "id": "ff81aa1ad30d5088"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T14:18:09.192561Z",
     "start_time": "2024-07-03T14:18:07.269648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "path_dataframe = '/Volumes/Seagate Bas/Vito/ML/Dataset/'\n",
    "path_multiclass = path_dataframe + 'MulticlassDatasets/'\n",
    "df_name = 'ncbi-virus-all-complete-nucleotides-2023-07-22'\n",
    "\n",
    "# if we want to load the file from parquet or from jsonl\n",
    "parquet = False\n",
    "\n",
    "# number of rows selected\n",
    "n = 200000"
   ],
   "id": "f7543ecb4132e51e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To reduce the dataset size we take the first 500 000 rows of the dataset",
   "id": "aa6a470e0dc2ee9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-03T13:57:07.559073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not glob.glob(path_dataframe + f'{df_name}.jsonl'):\n",
    "    print('Downloading dataset...')\n",
    "    # load dataset from hugging face\n",
    "    df = load_dataset(\"LKarlo/ncbi-virus-complete-dna-v230722\")['train']\n",
    "    # Save the dataset on parquet extention\n",
    "    df.to_parquet(path_dataframe + f'{df_name}.parquet')\n",
    "    df = df.sample(n=n)\n",
    "else: \n",
    "    print('Dataset already exists')\n",
    "    if os.path.exists(path_dataframe + f'ncbi-virus-{n}-dna.parquet') and parquet:\n",
    "        print(f'Loading the dataset consisting of {n} rows in parquet format')\n",
    "        df = pd.read_parquet(path_dataframe + f'ncbi-virus-{n}-dna.parquet')\n",
    "    else:\n",
    "        if os.path.exists(path_dataframe + f'ncbi-virus-{n}-dna.jsonl'):\n",
    "            print(f'Loading the dataset consisting of {n} rows in jsonl format')\n",
    "            df = None\n",
    "            with open(path_dataframe + f'ncbi-virus-{n}-dna.jsonl') as f:\n",
    "                for index, line in tqdm(enumerate(f)):\n",
    "                    data = json.loads(line)\n",
    "                    if df is None:\n",
    "                        df = pd.DataFrame(data=data, index=[0])\n",
    "                    else:\n",
    "                        df.loc[len(df)] = data\n",
    "        else:    \n",
    "            print(f'Loading {n} raws from the entire dataset from jsonl...')\n",
    "            df = None\n",
    "            with open(path_dataframe + f'{df_name}.jsonl') as f:\n",
    "                for index, line in tqdm(enumerate(f)):\n",
    "                    if index == n:\n",
    "                        break\n",
    "                    data = json.loads(line)\n",
    "                    if df is None:\n",
    "                        df = pd.DataFrame(data=data, index=[0])\n",
    "                    else:\n",
    "                        df.loc[len(df)] = data\n",
    "df.head()                "
   ],
   "id": "3e606c890db3f2f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n",
      "Loading 200000 raws from the entire dataset from jsonl...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the first 500 000 row of the dataset and save it",
   "id": "a103835611070246"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.exists(path_dataframe + f'ncbi-virus-{n}-dna.*'):\n",
    "    # Save parquet file in local\n",
    "    print('Saving subset of dataset...')\n",
    "    df.to_json(path_dataframe+f'ncbi-virus-{n}-dna.jsonl', index=False, lines=True)\n",
    "    df.to_parquet(path_dataframe+f'ncbi-virus-{n}-dna.parquet')"
   ],
   "id": "37bb4559688d7105",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## For the Multiclass classification task we analize the distribution of the feature target",
   "id": "f9c40e279f3d0257"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['Family'].value_counts()",
   "id": "c1aeb6d84297e448",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let’s delete the rows that contain nan to the family",
   "id": "1eda6fd36109a700"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:26:03.190096Z",
     "start_time": "2024-07-03T15:26:03.189750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = df[df['Family'] == 'nan'].index\n",
    "df.drop(index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ],
   "id": "2bfa3fed80dc024a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:26:03.193860Z",
     "start_time": "2024-07-03T15:26:03.193506Z"
    }
   },
   "cell_type": "code",
   "source": "df['Family'].value_counts()",
   "id": "c262270f1a132e5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To reduce the variability of sequences, let’s consider a maximum limit of 4000 elements per sequence",
   "id": "641718f847495bf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_len  = 4000\n",
    "\n",
    "df['Length'] = df['Length'].apply(lambda x: int(x))\n",
    "df = df.where(df[\"Length\"] < max_len)\n",
    "df['Family'].value_counts()"
   ],
   "id": "c7c136a73e261610",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### We select the 4 most frequent classes\n",
    "We select Geminiviridae, Spinareoviridae, Phenuiviridae and Circoviridae, and save the dataset both to make an unbalanced classification and to oversample the dataset."
   ],
   "id": "d6178789a0387e93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selected_df = df.query(\n",
    "    'Family == \"Geminiviridae\" or '\n",
    "    'Family == \"Spinareoviridae\" or '\n",
    "    'Family == \"Phenuiviridae\" or '\n",
    "    'Family == \"Circoviridae\"'\n",
    ")"
   ],
   "id": "b23cec5db573e935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:26:03.204167Z",
     "start_time": "2024-07-03T15:26:03.203831Z"
    }
   },
   "cell_type": "code",
   "source": "selected_df['Family'].value_counts()",
   "id": "41860793c7af7221",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We check if the dataset has already been saved and save it",
   "id": "818a5212536bedb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.exists(path_multiclass+'/UnbalancedDataset.parquet'):\n",
    "    print('Saving the dataset...')\n",
    "    selected_df.to_parquet(path_multiclass + '/UnbalancedDataset.parquet')\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('Dataset already exists')"
   ],
   "id": "17d247f567b2885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We perform the oversampling with the SMOTEN technique.\n",
    "\n",
    "The SMOTEN technique is an oversampling technique of minority classes that also works with categorical variables, the imblearn library containing its implementation is used, the reference paper talks about the SMOTE technique in general.\n",
    "\n",
    "The reference link is the above: [SMOTEN](https://doi.org/10.48550/arXiv.1106.1813) "
   ],
   "id": "82a211f0b89d5413"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = selected_df.columns.values.tolist()\n",
    "cols.remove('Family')\n",
    "y = selected_df['Family']\n",
    "X = selected_df[cols] \n",
    "resampler = SMOTEN(random_state=42)\n",
    "\n",
    "X,y = resampler.fit_resample(X,y)"
   ],
   "id": "d4430485c03d0400",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X['Family'] = y\n",
    "\n",
    "X['Family'].value_counts()"
   ],
   "id": "82dcfda828a61a0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X.tail()",
   "id": "30c747f99e30337a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.exists(path_multiclass+'/BalancedDataset.parquet'):\n",
    "    print('Saving the dataset...')\n",
    "    X.to_parquet(path_multiclass + '/BalancedDataset.parquet')\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('Dataset already exists')"
   ],
   "id": "fbfd97711be476f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
